{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a good model\n",
    "\n",
    "## Candidate algorithms: \n",
    "\n",
    "### 1. logistic regression\n",
    "- This is suitable for binary classification and is the litmus test for testing data's linearity, which I am not sure of.\n",
    "- I will drop some features with missing data points.\n",
    "\n",
    "### 2. Random forests\n",
    "- Random forest classification can deal well with categorical data such as the `PRODUCTÂ´ feature. \n",
    "- It will also not require any One-Hot-Encoding, and adding that would actually introduce sparsity we don't want.\n",
    "- This is also a good candidate for improving recall in imbalanced classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /usr/local/lib/python3.9/site-packages (1.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.9/site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.9/site-packages (from pandas) (2020.4)\n",
      "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.9/site-packages (from pandas) (1.19.4)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/site-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.1.1; however, version 21.3 is available.\n",
      "You should consider upgrading via the '/usr/local/opt/python@3.9/bin/python3.9 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.9/site-packages (1.19.4)\n",
      "\u001b[33mWARNING: You are using pip version 21.1.1; however, version 21.3 is available.\n",
      "You should consider upgrading via the '/usr/local/opt/python@3.9/bin/python3.9 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: seaborn in /usr/local/lib/python3.9/site-packages (0.11.2)\n",
      "Requirement already satisfied: matplotlib>=2.2 in /usr/local/lib/python3.9/site-packages (from seaborn) (3.4.3)\n",
      "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.9/site-packages (from seaborn) (1.7.1)\n",
      "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.9/site-packages (from seaborn) (1.19.4)\n",
      "Requirement already satisfied: pandas>=0.23 in /usr/local/lib/python3.9/site-packages (from seaborn) (1.1.5)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/site-packages (from matplotlib>=2.2->seaborn) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/site-packages (from matplotlib>=2.2->seaborn) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.9/site-packages (from matplotlib>=2.2->seaborn) (2.8.1)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.9/site-packages (from matplotlib>=2.2->seaborn) (2.4.7)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.9/site-packages (from matplotlib>=2.2->seaborn) (8.3.2)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.9/site-packages (from cycler>=0.10->matplotlib>=2.2->seaborn) (1.15.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.9/site-packages (from pandas>=0.23->seaborn) (2020.4)\n",
      "\u001b[33mWARNING: You are using pip version 21.1.1; however, version 21.3 is available.\n",
      "You should consider upgrading via the '/usr/local/opt/python@3.9/bin/python3.9 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas\n",
    "%pip install numpy\n",
    "%pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA # dimension reduction\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans # experimineting\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler #scaling\n",
    "\n",
    "from sklearn.pipeline import Pipeline # training pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier #1\n",
    "from sklearn.linear_model import LogisticRegression #2\n",
    "from sklearn.model_selection import GridSearchCV # optimization\n",
    "\n",
    "# reporting results\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, make_scorer, classification_report\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "# import seaborn as sns\n",
    "\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_rows\", None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('../problem/churn_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning / Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID                          False\n",
       "PRODUCT                     False\n",
       "CHURN_FLAG                  False\n",
       "TENURE                      False\n",
       "AVG_PLAY_120                False\n",
       "AVG_PLAY_90                  True\n",
       "SERIES_AVG_PLAY_LAST_120    False\n",
       "SERIES_AVG_PLAY_LAST_90      True\n",
       "FILM_AVG_PLAY_LAST_120      False\n",
       "FILM_AVG_PLAY_LAST_90        True\n",
       "dtype: bool"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DF Shapes\n",
      "(39121,)\n",
      "(39121, 5)\n"
     ]
    }
   ],
   "source": [
    " # separating the label from the data\n",
    "df_CHURN_LABEL = dataset.CHURN_FLAG\n",
    "\n",
    "# As disucssed in the data_exploration.ipynb, all *_90 features are dropped.\n",
    " \n",
    "df_DATA = dataset[['PRODUCT', 'TENURE', 'AVG_PLAY_120', 'SERIES_AVG_PLAY_LAST_120', 'FILM_AVG_PLAY_LAST_120']] # data to include in training with dropped columns\n",
    "df_DATA_no_NaN = df_DATA.fillna(df_DATA.mean())\n",
    "\n",
    "# checking that everything went alright\n",
    "print(\"DF Shapes\")\n",
    "print(df_CHURN_LABEL.shape)\n",
    "print(df_DATA.shape)\n",
    "\n",
    "# Adding one hot encoding since the product is a categorical feature\n",
    "one_hot_encoded_DATA = pd.get_dummies(df_DATA_no_NaN,prefix=['PRODUCT'], columns=['PRODUCT'])\n",
    "\n",
    "# making the data into ndarrays\n",
    "X, Y = np.array(df_DATA_no_NaN), np.array(df_CHURN_LABEL) \n",
    "X_OHE, Y_OHE = np.array(one_hot_encoded_DATA), np.array(df_CHURN_LABEL)\n",
    "\n",
    "# splitting the training and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.25, shuffle=True) # without one hot encoding\n",
    "X_train_OHE, X_test_OHE, y_train_OHE, y_test_OHE =  train_test_split(X_OHE, Y_OHE, test_size=0.25, shuffle=True) # with one hot encoding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameter (CV score=0.884):\n",
      "{'logistic__C': 0.0001, 'pca__n_components': 5}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00      1135\n",
      "           1       0.88      1.00      0.94      8646\n",
      "\n",
      "    accuracy                           0.88      9781\n",
      "   macro avg       0.44      0.50      0.47      9781\n",
      "weighted avg       0.78      0.88      0.83      9781\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "pca = PCA()\n",
    "logistic = LogisticRegression(max_iter=10000, tol=1e-4, penalty='l2')\n",
    "pipe = Pipeline(steps=[('pca', pca), ('standardscaler', StandardScaler()), ('logistic', logistic)])\n",
    "param_grid = {\n",
    "    'pca__n_components': [5, 15, 30, 45, 64],\n",
    "    'logistic__C': np.logspace(-4, 4, 4),\n",
    "}\n",
    "search = GridSearchCV(pipe, param_grid, n_jobs=-1)\n",
    "search.fit(X_train_OHE, y_train_OHE)\n",
    "print(\"Best parameter (CV score=%0.3f):\" % search.best_score_)\n",
    "print(search.best_params_)\n",
    "\n",
    "y_pred = search.predict(X_test_OHE)\n",
    "print(classification_report(y_test_OHE, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can see from above that this is a bad choice. Given that the majority of our rows have a `CHURN_FLAG` label of `1`, it's entirely possible to get a relatively high accuracy from the first try (such as `89%`) but with a very small macro f1-score (of `44%`).\n",
    "\n",
    "#### This is a sign of over-fitting, despite having removed redundant features, performed one hot encoding, and added L2 regularization.\n",
    "\n",
    "#### This model should therefore be eliminated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rfc(X_train_set, y_train_set, X_test_set, y_test_set, max_depth=40, random_state=0, min_samples_leaf = 3, min_samples_split = 4, n_estimators = 200, criterion='gini', max_features='auto', class_weight=None):\n",
    "    random_forest = RandomForestClassifier(max_depth=max_depth, random_state=random_state, min_samples_leaf=min_samples_leaf, min_samples_split=min_samples_split, n_estimators=n_estimators, criterion=criterion, max_features=max_features, class_weight=class_weight)\n",
    "    model = random_forest.fit(X_train_set, y_train_set.ravel())\n",
    "\n",
    "    y_pred = model.score(X_test_set, y_test_set)\n",
    "\n",
    "    acc_model = round(model.score(X_test_set, y_test_set) * 100, 2)\n",
    "    print(\"Model Accuracy:\", round(acc_model,2,), \"%\")\n",
    "    y_pred = model.predict(X_test_set)\n",
    "    print(classification_report(y_test_set, y_pred))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using the scikit-learn Random Forest classifier already gives us better results, as seen below:\n",
    "\n",
    "#### However, it is clear that recall for `CHURN_FLAG`=`0` needs to be improved in both precision and recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 80.84 %\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.29      0.46      0.35      1114\n",
      "           1       0.92      0.85      0.89      8667\n",
      "\n",
      "    accuracy                           0.81      9781\n",
      "   macro avg       0.61      0.66      0.62      9781\n",
      "weighted avg       0.85      0.81      0.83      9781\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# scaling data \n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train, y_train)\n",
    "X_test_scaled = scaler.fit_transform(X_test, y_test)\n",
    "\n",
    "rfc_scaled = rfc(X_train_scaled, y_train, X_test_scaled, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After taking a quick look at the most important features for this random forest classifier, we find that the `PRODUCT` feature has the lowest importance score of all.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='feature'>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "importances = pd.DataFrame({'feature':df_DATA.columns,'importance':np.round(rfc_scaled.feature_importances_,3)})\n",
    "importances = importances.sort_values('importance',ascending=False).set_index('feature')\n",
    "importances.head(15)\n",
    "importances[:15].plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One option to impvoe recall could be to remove the `PRODUCT` feature. It did (from `48%` to `53%`); but, as shown below, there was quite a decrease in precision. Our macro-averaged F1-score has also dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 68.61 %\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.18      0.51      0.27      1114\n",
      "           1       0.92      0.71      0.80      8667\n",
      "\n",
      "    accuracy                           0.69      9781\n",
      "   macro avg       0.55      0.61      0.54      9781\n",
      "weighted avg       0.83      0.69      0.74      9781\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Random forest classifier with product removed\n",
    "rfc_scaled_no_product = rfc(X_train_scaled[:, 1:], y_train, X_test_scaled[:, 1:], y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Another way to improve the score would be to use the One Hot Encoded `PRODUCT` feature. \n",
    "\n",
    "### This seems to significantly improve precision while creating a drop in recall, though both are more balanced now.\n",
    "### The overall accuracy and macro f1-scores are higher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 86.19 %\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.41      0.43      0.42      1135\n",
      "           1       0.92      0.92      0.92      8646\n",
      "\n",
      "    accuracy                           0.86      9781\n",
      "   macro avg       0.67      0.67      0.67      9781\n",
      "weighted avg       0.86      0.86      0.86      9781\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train_OHE_scaled = scaler.fit_transform(X_train_OHE, y_train)\n",
    "X_test_OHE_scaled = scaler.fit_transform(X_test_OHE, y_test)\n",
    "\n",
    "rfc_scaled_no_product_OHE = rfc(X_train_OHE_scaled, y_train_OHE, X_test_OHE_scaled, y_test_OHE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With the latest changes, training the model on a scaled one-hot-encoded dataset has yielded significantly higher precision, but recall remains a problem. This is mainly due to the class imbalance problem.\n",
    "\n",
    "#### To counter this, `sklearn.ensemble.RandomForestClassifier` offers a `class_weight` parameter that associates different weights to different labels. In our case, `CHURN_FLAG=0` is underrepresented.\n",
    "\n",
    "#### We can achieve better results by increasing its weight proportional to `CHURN_FLAG=1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 84.92 %\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.38      0.45      0.41      1135\n",
      "           1       0.93      0.90      0.91      8646\n",
      "\n",
      "    accuracy                           0.85      9781\n",
      "   macro avg       0.65      0.68      0.66      9781\n",
      "weighted avg       0.86      0.85      0.86      9781\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_test_OHE_scaled = scaler.fit_transform(X_test_OHE, y_test_OHE)\n",
    "\n",
    "rfc_scaled_no_product_OHE_weighted = rfc(X_train_OHE_scaled, y_train_OHE, X_test_OHE_scaled, y_test_OHE, class_weight={0: 2.5, 1: 0.9})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To find the absolute best model we could get, we can create a grid parameter dictionary focusing on the main hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "grid_param = {\n",
    "    'n_estimators': [100, 300, 500, 800, 1000],\n",
    "    'class_weight': [{0: 2, 1: 1}, {0: 2.5, 1: 0.9}, {0: 3, 1: 1}, {0: 4, 1: 1}, {0: 5, 1: 1}],\n",
    "    'max_depth': [20, 30, 40, 50, 60]\n",
    "}\n",
    "\n",
    "random_forest = RandomForestClassifier(random_state=0, min_samples_leaf = 3, min_samples_split = 4)\n",
    "\n",
    "gd_sr = GridSearchCV(estimator=random_forest,\n",
    "                     param_grid=grid_param,\n",
    "                     scoring={'accuracy': make_scorer(accuracy_score), 'precision': make_scorer(precision_score), 'recall': make_scorer(recall_score), 'f1_macro': make_scorer(f1_score)},\n",
    "                     refit='f1_macro',\n",
    "                     cv=5,\n",
    "                     n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=RandomForestClassifier(min_samples_leaf=3,\n",
       "                                              min_samples_split=4,\n",
       "                                              random_state=0),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'class_weight': [{0: 2, 1: 1}, {0: 2.5, 1: 0.9},\n",
       "                                          {0: 3, 1: 1}, {0: 4, 1: 1},\n",
       "                                          {0: 5, 1: 1}],\n",
       "                         'max_depth': [20, 30, 40, 50, 60],\n",
       "                         'n_estimators': [100, 300, 500, 800, 1000]},\n",
       "             refit='f1_macro',\n",
       "             scoring={'accuracy': make_scorer(accuracy_score),\n",
       "                      'f1_macro': make_scorer(f1_score),\n",
       "                      'precision': make_scorer(precision_score),\n",
       "                      'recall': make_scorer(recall_score)})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!# Takes approx. 40 minutes to run\n",
    "\n",
    "gd_sr.fit(X_train_OHE_scaled, y_train_OHE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(class_weight={0: 2, 1: 1}, max_depth=20,\n",
      "                       min_samples_leaf=3, min_samples_split=4,\n",
      "                       n_estimators=500, random_state=0)\n",
      "{'class_weight': {0: 2, 1: 1}, 'max_depth': 20, 'n_estimators': 500}\n",
      "0.954308926416577\n"
     ]
    }
   ],
   "source": [
    "print(gd_sr.best_estimator_) \n",
    "print(gd_sr.best_params_) # {'class_weight': {0: 2, 1: 1}, 'max_depth': 20, 'n_estimators': 500}\n",
    "print(gd_sr.best_score_) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 85.51 %\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.39      0.44      0.41      1135\n",
      "           1       0.93      0.91      0.92      8646\n",
      "\n",
      "    accuracy                           0.86      9781\n",
      "   macro avg       0.66      0.67      0.67      9781\n",
      "weighted avg       0.86      0.86      0.86      9781\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# training the best model decided by GridSearchCV\n",
    "best_random_forest = RandomForestClassifier(random_state=0, min_samples_leaf = 3, min_samples_split = 4, class_weight={0: 2, 1: 1}, max_depth=20, n_estimators=500)\n",
    "best_model = best_random_forest.fit(X_train_OHE_scaled, y_train_OHE.ravel())\n",
    "\n",
    "y_pred = best_model.score(X_test_OHE_scaled, y_test_OHE)\n",
    "\n",
    "acc_model = round(best_model.score(X_test_OHE_scaled, y_test_OHE) * 100, 2)\n",
    "print(\"Model Accuracy:\", round(acc_model,2,), \"%\")\n",
    "y_pred = best_model.predict(X_test_OHE_scaled)\n",
    "print(classification_report(y_test_OHE, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92.0 %\n"
     ]
    }
   ],
   "source": [
    "# our cross-validation score is much higher, but is also trained on a bigger amount of data\n",
    "\n",
    "X_OHE_scaled = scaler.fit_transform(X_OHE, Y_OHE)\n",
    "\n",
    "best_random_forest_cv = RandomForestClassifier(random_state=0, min_samples_leaf = 3, min_samples_split = 4, class_weight={0: 2, 1: 1}, max_depth=40, n_estimators=800)\n",
    "all_accuracies = np.mean(cross_val_score(estimator=best_random_forest_cv, X=X_OHE, y=Y_OHE, cv=10))\n",
    "print(round(all_accuracies,2,)*100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Is this model ready for production? It depends. \n",
    "\n",
    "### Precision and recall are very high for users who will churn. We would need to evaluate the cost of taking action to reduce churn with users who may have been false positives. If catching the biggest amount of churning users is what we are interested in, then we may not need to train the model further. \n",
    "\n",
    "### If, however, action is to be taken with non-churning users (to try to keep them for longer, etc), then we would need to reduce the class imbalance in the model and re-train it or choose another option altogether (such as a neural network, and so on). If the end-goal requires being able to accurately capture majority of users who will not churn (`CHURN_FLAG=0`), then we would aim to increase recall for that class. \n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
